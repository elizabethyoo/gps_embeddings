{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "want: \n",
    "- reproduce visualization from pptk tutorial\n",
    "- get summary statistics \n",
    "    - number of users \n",
    "\t- median/quartiles of number of locations visited "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages and dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from PIL import Image, ImageDraw\n",
    "import sys\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "import read_geolife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# testing reading data and assigning labels  for a single user \n",
    "df_all_users = read_geolife.read_all_users('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save raw data in pickle format \n",
    "df_valid_cb_valid_cb_valid_cb_all_users.to_pickle('../results/geolife.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pickled data\n",
    "df = pd.read_pickle('../results/geolife.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['alt'] == -3.264760e+04]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations: \n",
    "- latitude: falls outside of possible range e.g. min = 104.40, max = 400.17? \n",
    "\n",
    "- longitude: Ok at first glance\n",
    "\n",
    "- altitude: This is trickier. Codebook doesn't say what metric of altitude was used (w/r/t mean sea level? Some other reference point?) Many smartphones/GPS trackers will rely on satellite data (based on WGS84 https://en.wikipedia.org/wiki/World_Geodetic_System#A_new_World_Geodetic_System:_WGS_84) or measure altitude with air pressure (may not be accurate in pressure controlled areas e.g. inside an airplane).\n",
    "\n",
    "But also there is a value that corresponds to -30,000 ft below something -- that looks awfully low no matter what the metric is? Corresponds to one measurement from user 42. Could have been a measurement error... \n",
    "\n",
    "Codebook does say altitude values of '-777' are 'invalid'. At this point we have not processed them. \n",
    "\n",
    "The Earth's elevation point ranges from 1385 ft below sea level at the Dead Sea, and to 29035 ft at the summit of Mt. Everest. We will lower bound using the elevation at the Dead Sea, although judging from the threshold of -777 for invalid altitudes, that might be a generous threshold. \n",
    "\n",
    "We will also filter latitude values falling outside of the [-90 deg, 90 deg] range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter out \"unnrealistic values\". We'll have two filtered dataframes where one's\n",
    "# lower bounded by -777 (number given by codebook) and the other is lower bounded by the Dead Sea\n",
    "# and see how much data we lose out on. \n",
    "\n",
    "LOW_ALT = -1385\n",
    "INVAL_ALT = -777\n",
    "LOWER_LAT = -90\n",
    "UPPER_LAT = 90\n",
    "\n",
    "# lower bounded by codebook threshold\n",
    "df_valid_cb = df[(df['alt'] > INVAL_ALT) & (df['lat'] > LOWER_LAT) & (df['lat'] < UPPER_LAT)]\n",
    "# lower bounded by the Dead Sea\n",
    "df_valid_ds = df[(df['alt'] > LOW_ALT) & (df['lat'] > LOWER_LAT) & (df['lat'] < UPPER_LAT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_cb.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall summary\n",
    "df_valid_ds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-level summary\n",
    "pd.set_option('display.max_rows', None)\n",
    "df_valid_ds.groupby('user').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_cb.shape[0]/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codebook excerpt on features\n",
    "\n",
    "\"Line 1…6 are useless in this dataset, and can be ignored. Points are described in following lines, one for each line.\n",
    "Field 1: Latitude in decimal degrees.\n",
    "Field 2: Longitude in decimal degrees.\n",
    "Field 3: All set to 0 for this dataset.\n",
    "Field 4: Altitude in feet (-777 if not valid).\n",
    "Field 5: Date - number of days (with fractional part) that have passed since 12/30/1899.\n",
    "Field 6: Date as a string.\n",
    "Field 7: Time as a string.\n",
    "Note that field 5 and field 6&7 represent the same date/time in this dataset. You may use either of them.\n",
    "Example:\n",
    "39.906631,116.385564,0,492,40097.5864583333,2009-10-11,14:04:30\n",
    "39.906554,116.385625,0,492,40097.5865162037,2009-10-11,14:04:35\"\n",
    "\n",
    "Sanity checks on features i.e. known possible range\n",
    "Latitude: [-90 deg., +90 deg.]\n",
    "Longitutde: [-180 deg., 180 deg.]\n",
    "Altitude: \n",
    "https://en.wikipedia.org/wiki/List_of_lowest_airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "unique_users = df_valid_cb['user'].unique()[1:10]\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for user in unique_users:\n",
    "    user_data = df_valid_cb[df_valid_cb['user'] == user]\n",
    "    plt.plot(user_data['lon'], user_data['lat'], marker='o', label=f'User {user}')\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Trajectories of Multiple Users')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2024-02-20-tues\n",
    "TODO: \n",
    "- clean up notebook loll \n",
    "- continue cross referencing with the data documentation + keep filtering to deal with nonsensical values\n",
    "- discretize the surface covered by these gps coordinates (e.g. google s2 or something else) such that each element of that partition corresponds to a “location”\n",
    "- visualize trajectories some random subset of the users — do you know good Python packages for visualizing gps data? I was thinking Folium and GeoPandas — also this tutorial seems promising: https://courses.spatialthoughts.com/python-dataviz.html\n",
    "\n",
    "https://geopandas.org/en/stable/docs/user_guide/mapping.html\n",
    "https://python-visualization.github.io/folium/latest/getting_started.html\n",
    "\n",
    "Thanks for the suggestions! Looks like folium and geopandas are the way to go w/r/t visualization (and partitioning locations). I’ll check these out once I’m done cleaning the data and plot some sample trajectories. I’ll also generate some user-level summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "general processing gps data\n",
    "- https://jovian.com/jonpappalord/skmob03-preprocessing#C11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gps_embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
